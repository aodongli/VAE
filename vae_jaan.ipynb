{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "sg = tf.contrib.bayesflow.stochastic_graph\n",
    "st = tf.contrib.bayesflow.stochastic_tensor\n",
    "distributions = tf.contrib.distributions\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('data_dir', './dat/', 'Directory for data')\n",
    "flags.DEFINE_string('logdir', './log/', 'Directory for logs')\n",
    "\n",
    "# For making plots:\n",
    "flags.DEFINE_integer('latent_dim', 2, 'Latent dimensionality of model')\n",
    "flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "flags.DEFINE_integer('n_samples', 10, 'Number of samples to save')\n",
    "flags.DEFINE_integer('print_every', 10, 'Print every n iterations')\n",
    "flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "flags.DEFINE_integer('n_iterations', 1000, 'number of iterations')\n",
    "\n",
    "# For bigger model:\n",
    "# flags.DEFINE_integer('latent_dim', 100, 'Latent dimensionality of model')\n",
    "# flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "# flags.DEFINE_integer('n_samples', 1, 'Number of samples to save')\n",
    "# flags.DEFINE_integer('print_every', 1000, 'Print every n iterations')\n",
    "# flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "# flags.DEFINE_integer('n_iterations', 100000, 'number of iterations')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/t10k-labels-idx1-ubyte.gz\n",
      "Saving TensorBoard summaries and images to: /Users/owner/Desktop/interested courses/Deep Generative Models/vae/log/\n",
      "Iteration: 0 ELBO: -539.771 Examples/s: 2.428e+04\n",
      "Iteration: 10 ELBO: -304.046 Examples/s: 1.438e+05\n",
      "Iteration: 20 ELBO: -270.245 Examples/s: 1.354e+05\n",
      "Iteration: 30 ELBO: -253.799 Examples/s: 1.470e+05\n",
      "Iteration: 40 ELBO: -241.314 Examples/s: 1.031e+05\n",
      "Iteration: 50 ELBO: -221.608 Examples/s: 1.251e+05\n",
      "Iteration: 60 ELBO: -222.087 Examples/s: 1.503e+05\n",
      "Iteration: 70 ELBO: -204.375 Examples/s: 1.403e+05\n",
      "Iteration: 80 ELBO: -209.947 Examples/s: 1.595e+05\n",
      "Iteration: 90 ELBO: -207.843 Examples/s: 1.495e+05\n",
      "Iteration: 100 ELBO: -201.501 Examples/s: 1.504e+05\n",
      "Iteration: 110 ELBO: -192.818 Examples/s: 1.441e+05\n",
      "Iteration: 120 ELBO: -206.005 Examples/s: 1.607e+05\n",
      "Iteration: 130 ELBO: -201.393 Examples/s: 1.305e+05\n",
      "Iteration: 140 ELBO: -192.336 Examples/s: 1.525e+05\n",
      "Iteration: 150 ELBO: -189.067 Examples/s: 1.504e+05\n",
      "Iteration: 160 ELBO: -190.298 Examples/s: 1.465e+05\n",
      "Iteration: 170 ELBO: -180.815 Examples/s: 1.392e+05\n",
      "Iteration: 180 ELBO: -193.939 Examples/s: 1.527e+05\n",
      "Iteration: 190 ELBO: -178.399 Examples/s: 1.419e+05\n",
      "Iteration: 200 ELBO: -182.871 Examples/s: 1.452e+05\n",
      "Iteration: 210 ELBO: -190.424 Examples/s: 1.512e+05\n",
      "Iteration: 220 ELBO: -182.714 Examples/s: 1.513e+05\n",
      "Iteration: 230 ELBO: -182.174 Examples/s: 1.553e+05\n",
      "Iteration: 240 ELBO: -184.460 Examples/s: 1.084e+05\n",
      "Iteration: 250 ELBO: -175.859 Examples/s: 1.237e+05\n",
      "Iteration: 260 ELBO: -174.628 Examples/s: 1.514e+05\n",
      "Iteration: 270 ELBO: -188.145 Examples/s: 1.514e+05\n",
      "Iteration: 280 ELBO: -189.792 Examples/s: 1.525e+05\n",
      "Iteration: 290 ELBO: -175.316 Examples/s: 1.552e+05\n",
      "Iteration: 300 ELBO: -183.771 Examples/s: 1.503e+05\n",
      "Iteration: 310 ELBO: -169.477 Examples/s: 1.349e+05\n",
      "Iteration: 320 ELBO: -189.011 Examples/s: 1.546e+05\n",
      "Iteration: 330 ELBO: -188.086 Examples/s: 1.543e+05\n",
      "Iteration: 340 ELBO: -175.600 Examples/s: 1.485e+05\n",
      "Iteration: 350 ELBO: -169.149 Examples/s: 1.222e+05\n",
      "Iteration: 360 ELBO: -181.864 Examples/s: 1.283e+05\n",
      "Iteration: 370 ELBO: -181.370 Examples/s: 1.508e+05\n",
      "Iteration: 380 ELBO: -172.536 Examples/s: 1.488e+05\n",
      "Iteration: 390 ELBO: -176.736 Examples/s: 1.587e+05\n",
      "Iteration: 400 ELBO: -170.097 Examples/s: 1.402e+05\n",
      "Iteration: 410 ELBO: -177.477 Examples/s: 1.501e+05\n",
      "Iteration: 420 ELBO: -174.654 Examples/s: 1.517e+05\n",
      "Iteration: 430 ELBO: -174.079 Examples/s: 1.636e+05\n",
      "Iteration: 440 ELBO: -169.584 Examples/s: 1.465e+05\n",
      "Iteration: 450 ELBO: -175.508 Examples/s: 1.496e+05\n",
      "Iteration: 460 ELBO: -173.552 Examples/s: 1.477e+05\n",
      "Iteration: 470 ELBO: -168.971 Examples/s: 1.418e+05\n",
      "Iteration: 480 ELBO: -171.032 Examples/s: 1.564e+05\n",
      "Iteration: 490 ELBO: -178.375 Examples/s: 1.611e+05\n",
      "Iteration: 500 ELBO: -170.467 Examples/s: 1.539e+05\n",
      "Iteration: 510 ELBO: -178.728 Examples/s: 1.501e+05\n",
      "Iteration: 520 ELBO: -167.294 Examples/s: 1.559e+05\n",
      "Iteration: 530 ELBO: -170.079 Examples/s: 1.531e+05\n",
      "Iteration: 540 ELBO: -167.238 Examples/s: 1.470e+05\n",
      "Iteration: 550 ELBO: -174.328 Examples/s: 1.668e+05\n",
      "Iteration: 560 ELBO: -174.740 Examples/s: 1.481e+05\n",
      "Iteration: 570 ELBO: -170.325 Examples/s: 1.558e+05\n",
      "Iteration: 580 ELBO: -181.323 Examples/s: 1.473e+05\n",
      "Iteration: 590 ELBO: -172.988 Examples/s: 1.508e+05\n",
      "Iteration: 600 ELBO: -168.807 Examples/s: 1.587e+05\n",
      "Iteration: 610 ELBO: -167.896 Examples/s: 1.530e+05\n",
      "Iteration: 620 ELBO: -177.431 Examples/s: 9.251e+04\n",
      "Iteration: 630 ELBO: -179.145 Examples/s: 1.507e+05\n",
      "Iteration: 640 ELBO: -161.978 Examples/s: 1.492e+05\n",
      "Iteration: 650 ELBO: -173.639 Examples/s: 1.512e+05\n",
      "Iteration: 660 ELBO: -172.763 Examples/s: 1.533e+05\n",
      "Iteration: 670 ELBO: -164.246 Examples/s: 1.538e+05\n",
      "Iteration: 680 ELBO: -174.778 Examples/s: 1.563e+05\n",
      "Iteration: 690 ELBO: -167.813 Examples/s: 1.098e+05\n",
      "Iteration: 700 ELBO: -164.347 Examples/s: 1.551e+05\n",
      "Iteration: 710 ELBO: -163.105 Examples/s: 1.139e+05\n",
      "Iteration: 720 ELBO: -163.037 Examples/s: 1.489e+05\n",
      "Iteration: 730 ELBO: -173.531 Examples/s: 1.564e+05\n",
      "Iteration: 740 ELBO: -167.247 Examples/s: 1.488e+05\n",
      "Iteration: 750 ELBO: -170.142 Examples/s: 1.479e+05\n",
      "Iteration: 760 ELBO: -158.925 Examples/s: 1.445e+05\n",
      "Iteration: 770 ELBO: -163.285 Examples/s: 1.501e+05\n",
      "Iteration: 780 ELBO: -180.041 Examples/s: 1.561e+05\n",
      "Iteration: 790 ELBO: -170.585 Examples/s: 1.475e+05\n",
      "Iteration: 800 ELBO: -172.915 Examples/s: 1.404e+05\n",
      "Iteration: 810 ELBO: -172.026 Examples/s: 1.490e+05\n",
      "Iteration: 820 ELBO: -165.618 Examples/s: 1.402e+05\n",
      "Iteration: 830 ELBO: -173.629 Examples/s: 1.426e+05\n",
      "Iteration: 840 ELBO: -177.162 Examples/s: 1.460e+05\n",
      "Iteration: 850 ELBO: -159.117 Examples/s: 1.601e+05\n",
      "Iteration: 860 ELBO: -166.031 Examples/s: 1.540e+05\n",
      "Iteration: 870 ELBO: -173.428 Examples/s: 1.412e+05\n",
      "Iteration: 880 ELBO: -160.841 Examples/s: 1.511e+05\n",
      "Iteration: 890 ELBO: -169.477 Examples/s: 1.569e+05\n",
      "Iteration: 900 ELBO: -174.125 Examples/s: 1.551e+05\n",
      "Iteration: 910 ELBO: -176.814 Examples/s: 1.554e+05\n",
      "Iteration: 920 ELBO: -175.929 Examples/s: 1.541e+05\n",
      "Iteration: 930 ELBO: -160.938 Examples/s: 1.368e+05\n",
      "Iteration: 940 ELBO: -167.486 Examples/s: 1.428e+05\n",
      "Iteration: 950 ELBO: -161.464 Examples/s: 1.569e+05\n",
      "Iteration: 960 ELBO: -174.727 Examples/s: 1.502e+05\n",
      "Iteration: 970 ELBO: -168.689 Examples/s: 1.528e+05\n",
      "Iteration: 980 ELBO: -174.021 Examples/s: 1.550e+05\n",
      "Iteration: 990 ELBO: -167.009 Examples/s: 1.490e+05\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def inference_network(x, latent_dim, hidden_size):\n",
    "  \"\"\"Construct an inference network parametrizing a Gaussian.\n",
    "  Args:\n",
    "    x: A batch of MNIST digits.\n",
    "    latent_dim: The latent dimensionality.\n",
    "    hidden_size: The size of the neural net hidden layers.\n",
    "  Returns:\n",
    "    mu: Mean parameters for the variational family Normal\n",
    "    sigma: Standard deviation parameters for the variational family Normal\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.flatten(x)\n",
    "    #net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    gaussian_params = slim.fully_connected(\n",
    "        net, latent_dim * 2, activation_fn=None)\n",
    "  # The mean parameter is unconstrained\n",
    "  mu = gaussian_params[:, :latent_dim]\n",
    "  # The standard deviation must be positive. Parametrize with a softplus and\n",
    "  # add a small epsilon for numerical stability\n",
    "  #sigma = 1e-6 + tf.nn.softplus(gaussian_params[:, latent_dim:])\n",
    "  sigma = gaussian_params[:, latent_dim:]\n",
    "  return mu, sigma\n",
    "\n",
    "\n",
    "def generative_network(z, hidden_size):\n",
    "  \"\"\"Build a generative network parametrizing the likelihood of the data\n",
    "  Args:\n",
    "    z: Samples of latent variables\n",
    "    hidden_size: Size of the hidden state of the neural net\n",
    "  Returns:\n",
    "    bernoulli_logits: logits for the Bernoulli likelihood of the data\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.fully_connected(z, hidden_size)\n",
    "    '''\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    '''\n",
    "    bernoulli_logits = slim.fully_connected(net, 784, activation_fn=None)\n",
    "    bernoulli_logits = tf.reshape(bernoulli_logits, [-1, 28, 28, 1])\n",
    "  return bernoulli_logits\n",
    "\n",
    "\n",
    "def train():\n",
    "  # Train a Variational Autoencoder on MNIST\n",
    "\n",
    "  # Input placeholders\n",
    "  with tf.name_scope('data'):\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    tf.summary.image('data', x)\n",
    "\n",
    "  #with tf.variable_scope('variational'):\n",
    "    #q_mu, q_sigma = inference_network(x=x,\n",
    "                                      #latent_dim=FLAGS.latent_dim,\n",
    "                                      #hidden_size=FLAGS.hidden_size)\n",
    "    #with st.value_type(st.SampleValue()):\n",
    "      # The variational distribution is a Normal with mean and standard\n",
    "      # deviation given by the inference network\n",
    "      #q_z = st.StochasticTensor(distributions.Normal(loc=q_mu, scale=q_sigma))\n",
    "    \n",
    "\n",
    "  with tf.variable_scope('model'):\n",
    "    with tf.variable_scope('inference'):\n",
    "      q_mu, q_sigma = inference_network(x=x,\n",
    "                                        latent_dim=FLAGS.latent_dim,\n",
    "                                        hidden_size=FLAGS.hidden_size)\n",
    "      q_z = distributions.Normal(loc=q_mu, scale=q_sigma)\n",
    "    # The likelihood is Bernoulli-distributed with logits given by the\n",
    "    # generative network\n",
    "    p_x_given_z_logits = generative_network(z=q_z.sample(),\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    p_x_given_z = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    posterior_predictive_samples = p_x_given_z.sample()\n",
    "    tf.summary.image('posterior_predictive',\n",
    "                     tf.cast(posterior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    #logits = tf.ones()\n",
    "    #p_z = distribution.Categorical(logits=logits)\n",
    "    p_z = distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                               scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "    #p_z = distributions.Uniform(low=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                                #high=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "    p_z_sample = p_z.sample(FLAGS.n_samples)\n",
    "    p_x_given_z_logits = generative_network(z=p_z_sample,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_samples = prior_predictive.sample()\n",
    "    tf.summary.image('prior_predictive',\n",
    "                     tf.cast(prior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior with a placeholder\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
    "    p_x_given_z_logits = generative_network(z=z_input,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive_inp = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
    "\n",
    "  # Build the evidence lower bound (ELBO) or the negative loss\n",
    "  kl = tf.reduce_sum(distributions.kl(q_z, p_z), 1)\n",
    "  expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),\n",
    "                                          [1, 2, 3])\n",
    "\n",
    "  elbo = tf.reduce_sum(expected_log_likelihood - kl, 0)\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "  train_op = optimizer.minimize(-elbo)\n",
    "\n",
    "  # Merge all the summaries\n",
    "  summary_op = tf.summary.merge_all()\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n",
    "\n",
    "  # Run training\n",
    "  sess = tf.InteractiveSession()\n",
    "  sess.run(init_op)\n",
    "\n",
    "  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "  print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
    "  train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
    "\n",
    "  # Get fixed MNIST digits for plotting posterior means during training\n",
    "  np_x_fixed, np_y = mnist.test.next_batch(5000)\n",
    "  np_x_fixed = np_x_fixed.reshape(5000, 28, 28, 1)\n",
    "  np_x_fixed = (np_x_fixed > 0.5).astype(np.float32)\n",
    "\n",
    "  for i in range(FLAGS.n_iterations):\n",
    "    # Re-binarize the data at every batch; this improves results\n",
    "    np_x, _ = mnist.train.next_batch(FLAGS.batch_size)\n",
    "    np_x = np_x.reshape(FLAGS.batch_size, 28, 28, 1)\n",
    "    np_x = (np_x > 0.5).astype(np.float32)\n",
    "    sess.run(train_op, {x: np_x})\n",
    "\n",
    "    # Print progress and save samples every so often\n",
    "    t0 = time.time()\n",
    "    if i % FLAGS.print_every == 0:\n",
    "      np_elbo, summary_str = sess.run([elbo, summary_op], {x: np_x})\n",
    "      train_writer.add_summary(summary_str, i)\n",
    "      print('Iteration: {0:d} ELBO: {1:.3f} Examples/s: {2:.3e}'.format(\n",
    "          i,\n",
    "          np_elbo / FLAGS.batch_size,\n",
    "          FLAGS.batch_size * FLAGS.print_every / (time.time() - t0)))\n",
    "      t0 = time.time()\n",
    "\n",
    "      # Save samples\n",
    "      np_posterior_samples, np_prior_samples = sess.run(\n",
    "          [posterior_predictive_samples, prior_predictive_samples], {x: np_x})\n",
    "      for k in range(FLAGS.n_samples):\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
    "        imsave(f_name, np_x[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
    "        imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
    "        imsave(f_name, np_prior_samples[k, :, :, 0])\n",
    "\n",
    "      # Plot the posterior predictive space\n",
    "      if FLAGS.latent_dim == 2:\n",
    "        np_q_mu = sess.run(q_mu, {x: np_x_fixed})\n",
    "        cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\"))\n",
    "        f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
    "        im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(np_y, 1), cmap=cmap,\n",
    "                        alpha=0.7)\n",
    "        ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
    "        ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
    "        ax.set_xlim([-10., 10.])\n",
    "        ax.set_ylim([-10., 10.])\n",
    "        f.colorbar(im, ax=ax, label='Digit class')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FLAGS.logdir,\n",
    "                                 'posterior_predictive_map_frame_%d.png' % i))\n",
    "        plt.close()\n",
    "\n",
    "        nx = ny = 20\n",
    "        x_values = np.linspace(-10, 10, nx)\n",
    "        y_values = np.linspace(-10, 10, ny)\n",
    "        canvas = np.empty((28 * ny, 28 * nx))\n",
    "        for ii, yi in enumerate(x_values):\n",
    "          for j, xi in enumerate(y_values):\n",
    "            np_z = np.array([[xi, yi]])\n",
    "            x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z})\n",
    "            canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
    "                   28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
    "        imsave(os.path.join(FLAGS.logdir,\n",
    "                            'prior_predictive_map_frame_%d.png' % i), canvas)\n",
    "        # plt.figure(figsize=(8, 10))\n",
    "        # Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "        # plt.imshow(canvas, origin=\"upper\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig()\n",
    "\n",
    "  # Make the gifs\n",
    "  if FLAGS.latent_dim == 2:\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/posterior_predictive_map_frame*png {0}/posterior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/prior_predictive_map_frame*png {0}/prior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.logdir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.logdir)\n",
    "  tf.gfile.MakeDirs(FLAGS.logdir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
