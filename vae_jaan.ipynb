{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.misc import imsave\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "\n",
    "sg = tf.contrib.bayesflow.stochastic_graph\n",
    "st = tf.contrib.bayesflow.stochastic_tensor\n",
    "distributions = tf.contrib.distributions\n",
    "\n",
    "\n",
    "flags = tf.app.flags\n",
    "flags.DEFINE_string('data_dir', '/Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/', 'Directory for data')\n",
    "flags.DEFINE_string('logdir', '/Users/owner/Desktop/interested courses/Deep Generative Models/vae/log/', 'Directory for logs')\n",
    "\n",
    "# For making plots:\n",
    "flags.DEFINE_integer('latent_dim', 2, 'Latent dimensionality of model')\n",
    "flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "flags.DEFINE_integer('n_samples', 10, 'Number of samples to save')\n",
    "flags.DEFINE_integer('print_every', 10, 'Print every n iterations')\n",
    "flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "flags.DEFINE_integer('n_iterations', 1000, 'number of iterations')\n",
    "\n",
    "# For bigger model:\n",
    "# flags.DEFINE_integer('latent_dim', 100, 'Latent dimensionality of model')\n",
    "# flags.DEFINE_integer('batch_size', 64, 'Minibatch size')\n",
    "# flags.DEFINE_integer('n_samples', 1, 'Number of samples to save')\n",
    "# flags.DEFINE_integer('print_every', 1000, 'Print every n iterations')\n",
    "# flags.DEFINE_integer('hidden_size', 200, 'Hidden size for neural networks')\n",
    "# flags.DEFINE_integer('n_iterations', 100000, 'number of iterations')\n",
    "\n",
    "FLAGS = flags.FLAGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/train-images-idx3-ubyte.gz\n",
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/train-labels-idx1-ubyte.gz\n",
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/t10k-images-idx3-ubyte.gz\n",
      "Extracting /Users/owner/Desktop/interested courses/Deep Generative Models/vae/dat/t10k-labels-idx1-ubyte.gz\n",
      "Saving TensorBoard summaries and images to: /Users/owner/Desktop/interested courses/Deep Generative Models/vae/log/\n",
      "Iteration: 0 ELBO: -542.936 Examples/s: 1.727e+04\n",
      "Iteration: 10 ELBO: -239.450 Examples/s: 9.389e+04\n",
      "Iteration: 20 ELBO: -215.263 Examples/s: 1.207e+05\n",
      "Iteration: 30 ELBO: -207.835 Examples/s: 8.256e+04\n",
      "Iteration: 40 ELBO: -203.034 Examples/s: 7.946e+04\n",
      "Iteration: 50 ELBO: -193.634 Examples/s: 1.124e+05\n",
      "Iteration: 60 ELBO: -208.094 Examples/s: 5.809e+04\n",
      "Iteration: 70 ELBO: -204.605 Examples/s: 1.068e+05\n",
      "Iteration: 80 ELBO: -207.358 Examples/s: 1.190e+05\n",
      "Iteration: 90 ELBO: -202.106 Examples/s: 1.194e+05\n",
      "Iteration: 100 ELBO: -197.975 Examples/s: 1.157e+05\n",
      "Iteration: 110 ELBO: -204.818 Examples/s: 1.152e+05\n",
      "Iteration: 120 ELBO: -202.317 Examples/s: 1.128e+05\n",
      "Iteration: 130 ELBO: -197.007 Examples/s: 1.212e+05\n",
      "Iteration: 140 ELBO: -191.871 Examples/s: 9.741e+04\n",
      "Iteration: 150 ELBO: -191.110 Examples/s: 1.172e+05\n",
      "Iteration: 160 ELBO: -192.105 Examples/s: 1.129e+05\n",
      "Iteration: 170 ELBO: -194.623 Examples/s: 1.124e+05\n",
      "Iteration: 180 ELBO: -184.526 Examples/s: 1.210e+05\n",
      "Iteration: 190 ELBO: -190.679 Examples/s: 1.210e+05\n",
      "Iteration: 200 ELBO: -185.471 Examples/s: 1.088e+05\n",
      "Iteration: 210 ELBO: -192.705 Examples/s: 9.136e+04\n",
      "Iteration: 220 ELBO: -192.020 Examples/s: 7.593e+04\n",
      "Iteration: 230 ELBO: -200.849 Examples/s: 1.178e+05\n",
      "Iteration: 240 ELBO: -174.094 Examples/s: 1.142e+05\n",
      "Iteration: 250 ELBO: -198.380 Examples/s: 1.106e+05\n",
      "Iteration: 260 ELBO: -179.841 Examples/s: 1.229e+05\n",
      "Iteration: 270 ELBO: -183.643 Examples/s: 1.179e+05\n",
      "Iteration: 280 ELBO: -192.854 Examples/s: 1.208e+05\n",
      "Iteration: 290 ELBO: -188.680 Examples/s: 1.174e+05\n",
      "Iteration: 300 ELBO: -193.153 Examples/s: 1.154e+05\n",
      "Iteration: 310 ELBO: -180.477 Examples/s: 1.130e+05\n",
      "Iteration: 320 ELBO: -177.652 Examples/s: 1.094e+05\n",
      "Iteration: 330 ELBO: -186.140 Examples/s: 1.172e+05\n",
      "Iteration: 340 ELBO: -182.442 Examples/s: 1.141e+05\n",
      "Iteration: 350 ELBO: -188.890 Examples/s: 1.265e+05\n",
      "Iteration: 360 ELBO: -207.373 Examples/s: 1.094e+05\n",
      "Iteration: 370 ELBO: -186.383 Examples/s: 1.163e+05\n",
      "Iteration: 380 ELBO: -182.775 Examples/s: 1.222e+05\n",
      "Iteration: 390 ELBO: -188.055 Examples/s: 1.206e+05\n",
      "Iteration: 400 ELBO: -187.157 Examples/s: 1.195e+05\n",
      "Iteration: 410 ELBO: -182.772 Examples/s: 1.248e+05\n",
      "Iteration: 420 ELBO: -174.935 Examples/s: 1.207e+05\n",
      "Iteration: 430 ELBO: -178.123 Examples/s: 1.250e+05\n",
      "Iteration: 440 ELBO: -189.200 Examples/s: 1.176e+05\n",
      "Iteration: 450 ELBO: -180.473 Examples/s: 1.185e+05\n",
      "Iteration: 460 ELBO: -191.134 Examples/s: 1.280e+05\n",
      "Iteration: 470 ELBO: -188.326 Examples/s: 9.611e+04\n",
      "Iteration: 480 ELBO: -177.224 Examples/s: 1.200e+05\n",
      "Iteration: 490 ELBO: -175.671 Examples/s: 8.349e+04\n",
      "Iteration: 500 ELBO: -192.962 Examples/s: 1.227e+05\n",
      "Iteration: 510 ELBO: -178.603 Examples/s: 1.115e+05\n",
      "Iteration: 520 ELBO: -168.268 Examples/s: 9.080e+04\n",
      "Iteration: 530 ELBO: -176.255 Examples/s: 1.150e+05\n",
      "Iteration: 540 ELBO: -195.871 Examples/s: 1.209e+05\n",
      "Iteration: 550 ELBO: -165.837 Examples/s: 1.231e+05\n",
      "Iteration: 560 ELBO: -174.417 Examples/s: 8.003e+04\n",
      "Iteration: 570 ELBO: -175.749 Examples/s: 1.160e+05\n",
      "Iteration: 580 ELBO: -168.069 Examples/s: 8.963e+04\n",
      "Iteration: 590 ELBO: -175.474 Examples/s: 8.072e+04\n",
      "Iteration: 600 ELBO: -163.831 Examples/s: 1.206e+05\n",
      "Iteration: 610 ELBO: -178.627 Examples/s: 1.144e+05\n",
      "Iteration: 620 ELBO: -177.977 Examples/s: 8.887e+04\n",
      "Iteration: 630 ELBO: -183.570 Examples/s: 1.135e+05\n",
      "Iteration: 640 ELBO: -160.303 Examples/s: 1.111e+05\n",
      "Iteration: 650 ELBO: -171.775 Examples/s: 1.156e+05\n",
      "Iteration: 660 ELBO: -162.850 Examples/s: 1.163e+05\n",
      "Iteration: 670 ELBO: -164.287 Examples/s: 1.048e+05\n",
      "Iteration: 680 ELBO: -166.597 Examples/s: 6.528e+04\n",
      "Iteration: 690 ELBO: -169.188 Examples/s: 1.208e+05\n",
      "Iteration: 700 ELBO: -168.622 Examples/s: 1.014e+05\n",
      "Iteration: 710 ELBO: -172.218 Examples/s: 1.234e+05\n",
      "Iteration: 720 ELBO: -175.424 Examples/s: 1.161e+05\n",
      "Iteration: 730 ELBO: -167.786 Examples/s: 1.188e+05\n",
      "Iteration: 740 ELBO: -158.432 Examples/s: 9.807e+04\n",
      "Iteration: 750 ELBO: -157.886 Examples/s: 1.157e+05\n",
      "Iteration: 760 ELBO: -161.654 Examples/s: 1.109e+05\n",
      "Iteration: 770 ELBO: -169.133 Examples/s: 1.033e+05\n",
      "Iteration: 780 ELBO: -164.161 Examples/s: 1.144e+05\n",
      "Iteration: 790 ELBO: -167.655 Examples/s: 1.220e+05\n",
      "Iteration: 800 ELBO: -160.658 Examples/s: 1.018e+05\n",
      "Iteration: 810 ELBO: -175.894 Examples/s: 6.636e+04\n",
      "Iteration: 820 ELBO: -172.846 Examples/s: 1.086e+05\n",
      "Iteration: 830 ELBO: -166.827 Examples/s: 1.217e+05\n",
      "Iteration: 840 ELBO: -166.819 Examples/s: 1.155e+05\n",
      "Iteration: 850 ELBO: -164.838 Examples/s: 1.164e+05\n",
      "Iteration: 860 ELBO: -151.475 Examples/s: 1.174e+05\n",
      "Iteration: 870 ELBO: -165.534 Examples/s: 6.953e+04\n",
      "Iteration: 880 ELBO: -164.562 Examples/s: 1.195e+05\n",
      "Iteration: 890 ELBO: -178.370 Examples/s: 1.074e+05\n",
      "Iteration: 900 ELBO: -152.355 Examples/s: 1.173e+05\n",
      "Iteration: 910 ELBO: -168.921 Examples/s: 1.233e+05\n",
      "Iteration: 920 ELBO: -145.152 Examples/s: 1.169e+05\n",
      "Iteration: 930 ELBO: -159.398 Examples/s: 8.986e+04\n",
      "Iteration: 940 ELBO: -169.799 Examples/s: 7.401e+04\n",
      "Iteration: 950 ELBO: -153.854 Examples/s: 1.105e+05\n",
      "Iteration: 960 ELBO: -154.200 Examples/s: 1.111e+05\n",
      "Iteration: 970 ELBO: -158.151 Examples/s: 1.122e+05\n",
      "Iteration: 980 ELBO: -151.400 Examples/s: 1.267e+05\n",
      "Iteration: 990 ELBO: -153.954 Examples/s: 1.164e+05\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def inference_network(x, latent_dim, hidden_size):\n",
    "  \"\"\"Construct an inference network parametrizing a Gaussian.\n",
    "  Args:\n",
    "    x: A batch of MNIST digits.\n",
    "    latent_dim: The latent dimensionality.\n",
    "    hidden_size: The size of the neural net hidden layers.\n",
    "  Returns:\n",
    "    mu: Mean parameters for the variational family Normal\n",
    "    sigma: Standard deviation parameters for the variational family Normal\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.flatten(x)\n",
    "    #net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    gaussian_params = slim.fully_connected(\n",
    "        net, latent_dim * 2, activation_fn=None)\n",
    "  # The mean parameter is unconstrained\n",
    "  mu = gaussian_params[:, :latent_dim]\n",
    "  # The standard deviation must be positive. Parametrize with a softplus and\n",
    "  # add a small epsilon for numerical stability\n",
    "  #sigma = 1e-6 + tf.nn.softplus(gaussian_params[:, latent_dim:])\n",
    "  sigma = gaussian_params[:, latent_dim:]\n",
    "  return mu, sigma\n",
    "\n",
    "\n",
    "def generative_network(z, hidden_size):\n",
    "  \"\"\"Build a generative network parametrizing the likelihood of the data\n",
    "  Args:\n",
    "    z: Samples of latent variables\n",
    "    hidden_size: Size of the hidden state of the neural net\n",
    "  Returns:\n",
    "    bernoulli_logits: logits for the Bernoulli likelihood of the data\n",
    "  \"\"\"\n",
    "  with slim.arg_scope([slim.fully_connected], activation_fn=tf.nn.relu):\n",
    "    net = slim.fully_connected(z, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    net = slim.fully_connected(net, hidden_size)\n",
    "    bernoulli_logits = slim.fully_connected(net, 784, activation_fn=None)\n",
    "    bernoulli_logits = tf.reshape(bernoulli_logits, [-1, 28, 28, 1])\n",
    "  return bernoulli_logits\n",
    "\n",
    "\n",
    "def train():\n",
    "  # Train a Variational Autoencoder on MNIST\n",
    "\n",
    "  # Input placeholders\n",
    "  with tf.name_scope('data'):\n",
    "    x = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "    tf.summary.image('data', x)\n",
    "\n",
    "  #with tf.variable_scope('variational'):\n",
    "    #q_mu, q_sigma = inference_network(x=x,\n",
    "                                      #latent_dim=FLAGS.latent_dim,\n",
    "                                      #hidden_size=FLAGS.hidden_size)\n",
    "    #with st.value_type(st.SampleValue()):\n",
    "      # The variational distribution is a Normal with mean and standard\n",
    "      # deviation given by the inference network\n",
    "      #q_z = st.StochasticTensor(distributions.Normal(loc=q_mu, scale=q_sigma))\n",
    "    \n",
    "\n",
    "  with tf.variable_scope('model'):\n",
    "    with tf.variable_scope('inference'):\n",
    "      q_mu, q_sigma = inference_network(x=x,\n",
    "                                        latent_dim=FLAGS.latent_dim,\n",
    "                                        hidden_size=FLAGS.hidden_size)\n",
    "      q_z = distributions.Normal(loc=q_mu, scale=q_sigma)\n",
    "    # The likelihood is Bernoulli-distributed with logits given by the\n",
    "    # generative network\n",
    "    p_x_given_z_logits = generative_network(z=q_z.sample(),\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    p_x_given_z = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    posterior_predictive_samples = p_x_given_z.sample()\n",
    "    tf.summary.image('posterior_predictive',\n",
    "                     tf.cast(posterior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    #logits = tf.ones()\n",
    "    #p_z = distribution.Categorical(logits=logits)\n",
    "    p_z = distributions.Normal(loc=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                               scale=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "    #p_z = distributions.Uniform(low=np.zeros(FLAGS.latent_dim, dtype=np.float32),\n",
    "                                #high=np.ones(FLAGS.latent_dim, dtype=np.float32))\n",
    "    p_z_sample = p_z.sample(FLAGS.n_samples)\n",
    "    p_x_given_z_logits = generative_network(z=p_z_sample,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_samples = prior_predictive.sample()\n",
    "    tf.summary.image('prior_predictive',\n",
    "                     tf.cast(prior_predictive_samples, tf.float32))\n",
    "\n",
    "  # Take samples from the prior with a placeholder\n",
    "  with tf.variable_scope('model', reuse=True):\n",
    "    z_input = tf.placeholder(tf.float32, [None, FLAGS.latent_dim])\n",
    "    p_x_given_z_logits = generative_network(z=z_input,\n",
    "                                            hidden_size=FLAGS.hidden_size)\n",
    "    prior_predictive_inp = distributions.Bernoulli(logits=p_x_given_z_logits)\n",
    "    prior_predictive_inp_sample = prior_predictive_inp.sample()\n",
    "\n",
    "  # Build the evidence lower bound (ELBO) or the negative loss\n",
    "  kl = tf.reduce_sum(distributions.kl(q_z, p_z), 1)\n",
    "  expected_log_likelihood = tf.reduce_sum(p_x_given_z.log_prob(x),\n",
    "                                          [1, 2, 3])\n",
    "\n",
    "  elbo = tf.reduce_sum(expected_log_likelihood - kl, 0)\n",
    "\n",
    "  optimizer = tf.train.RMSPropOptimizer(learning_rate=0.001)\n",
    "\n",
    "  train_op = optimizer.minimize(-elbo)\n",
    "\n",
    "  # Merge all the summaries\n",
    "  summary_op = tf.summary.merge_all()\n",
    "\n",
    "  init_op = tf.global_variables_initializer()\n",
    "\n",
    "  # Run training\n",
    "  sess = tf.InteractiveSession()\n",
    "  sess.run(init_op)\n",
    "\n",
    "  mnist = read_data_sets(FLAGS.data_dir, one_hot=True)\n",
    "\n",
    "  print('Saving TensorBoard summaries and images to: %s' % FLAGS.logdir)\n",
    "  train_writer = tf.summary.FileWriter(FLAGS.logdir, sess.graph)\n",
    "\n",
    "  # Get fixed MNIST digits for plotting posterior means during training\n",
    "  np_x_fixed, np_y = mnist.test.next_batch(5000)\n",
    "  np_x_fixed = np_x_fixed.reshape(5000, 28, 28, 1)\n",
    "  np_x_fixed = (np_x_fixed > 0.5).astype(np.float32)\n",
    "\n",
    "  for i in range(FLAGS.n_iterations):\n",
    "    # Re-binarize the data at every batch; this improves results\n",
    "    np_x, _ = mnist.train.next_batch(FLAGS.batch_size)\n",
    "    np_x = np_x.reshape(FLAGS.batch_size, 28, 28, 1)\n",
    "    np_x = (np_x > 0.5).astype(np.float32)\n",
    "    sess.run(train_op, {x: np_x})\n",
    "\n",
    "    # Print progress and save samples every so often\n",
    "    t0 = time.time()\n",
    "    if i % FLAGS.print_every == 0:\n",
    "      np_elbo, summary_str = sess.run([elbo, summary_op], {x: np_x})\n",
    "      train_writer.add_summary(summary_str, i)\n",
    "      print('Iteration: {0:d} ELBO: {1:.3f} Examples/s: {2:.3e}'.format(\n",
    "          i,\n",
    "          np_elbo / FLAGS.batch_size,\n",
    "          FLAGS.batch_size * FLAGS.print_every / (time.time() - t0)))\n",
    "      t0 = time.time()\n",
    "\n",
    "      # Save samples\n",
    "      np_posterior_samples, np_prior_samples = sess.run(\n",
    "          [posterior_predictive_samples, prior_predictive_samples], {x: np_x})\n",
    "      for k in range(FLAGS.n_samples):\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_data.jpg' % (i, k))\n",
    "        imsave(f_name, np_x[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_posterior_predictive_%d_sample.jpg' % (i, k))\n",
    "        imsave(f_name, np_posterior_samples[k, :, :, 0])\n",
    "        f_name = os.path.join(\n",
    "            FLAGS.logdir, 'iter_%d_prior_predictive_%d.jpg' % (i, k))\n",
    "        imsave(f_name, np_prior_samples[k, :, :, 0])\n",
    "\n",
    "      # Plot the posterior predictive space\n",
    "      if FLAGS.latent_dim == 2:\n",
    "        np_q_mu = sess.run(q_mu, {x: np_x_fixed})\n",
    "        cmap = mpl.colors.ListedColormap(sns.color_palette(\"husl\"))\n",
    "        f, ax = plt.subplots(1, figsize=(6 * 1.1618, 6))\n",
    "        im = ax.scatter(np_q_mu[:, 0], np_q_mu[:, 1], c=np.argmax(np_y, 1), cmap=cmap,\n",
    "                        alpha=0.7)\n",
    "        ax.set_xlabel('First dimension of sampled latent variable $z_1$')\n",
    "        ax.set_ylabel('Second dimension of sampled latent variable mean $z_2$')\n",
    "        ax.set_xlim([-10., 10.])\n",
    "        ax.set_ylim([-10., 10.])\n",
    "        f.colorbar(im, ax=ax, label='Digit class')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(FLAGS.logdir,\n",
    "                                 'posterior_predictive_map_frame_%d.png' % i))\n",
    "        plt.close()\n",
    "\n",
    "        nx = ny = 20\n",
    "        x_values = np.linspace(-10, 10, nx)\n",
    "        y_values = np.linspace(-10, 10, ny)\n",
    "        canvas = np.empty((28 * ny, 28 * nx))\n",
    "        for ii, yi in enumerate(x_values):\n",
    "          for j, xi in enumerate(y_values):\n",
    "            np_z = np.array([[xi, yi]])\n",
    "            x_mean = sess.run(prior_predictive_inp_sample, {z_input: np_z})\n",
    "            canvas[(nx - ii - 1) * 28:(nx - ii) * 28, j *\n",
    "                   28:(j + 1) * 28] = x_mean[0].reshape(28, 28)\n",
    "        imsave(os.path.join(FLAGS.logdir,\n",
    "                            'prior_predictive_map_frame_%d.png' % i), canvas)\n",
    "        # plt.figure(figsize=(8, 10))\n",
    "        # Xi, Yi = np.meshgrid(x_values, y_values)\n",
    "        # plt.imshow(canvas, origin=\"upper\")\n",
    "        # plt.tight_layout()\n",
    "        # plt.savefig()\n",
    "\n",
    "  # Make the gifs\n",
    "  if FLAGS.latent_dim == 2:\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/posterior_predictive_map_frame*png {0}/posterior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "    os.system(\n",
    "        'convert -delay 15 -loop 0 {0}/prior_predictive_map_frame*png {0}/prior_predictive.gif'\n",
    "        .format(FLAGS.logdir))\n",
    "\n",
    "\n",
    "def main(_):\n",
    "  if tf.gfile.Exists(FLAGS.logdir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.logdir)\n",
    "  tf.gfile.MakeDirs(FLAGS.logdir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
